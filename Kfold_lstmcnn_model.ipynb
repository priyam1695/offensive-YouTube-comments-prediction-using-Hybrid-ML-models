{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4640f2bd-c229-42a0-b180-e266d6958029",
    "_uuid": "5a8550ea-097d-403c-91c1-3c3b5244d194"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\"\"\"from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, Dense, Embedding, Dropout,MaxPooling1D, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report, f1_score\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "import tensorflow \n",
    "from tensorflow.python.lib.io import file_io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc2d0fab-c59a-4c93-ad61-d30b1b6a9929",
    "_uuid": "c16df588-b323-4902-bf21-d53eb3520920"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/finaldata/YouTube_Dadvar_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dad535a7-3333-40e0-b027-4c77e5798f74",
    "_uuid": "9ae3ad68-a588-4a36-859d-049b6a0cda55"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "079ed963-a712-436a-9848-1da1832d71b0",
    "_uuid": "52364acd-c989-469c-a39d-be61ba89a97a"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8bc19bb0-5214-4089-ae08-aabd0438197d",
    "_uuid": "108950d8-dabe-49a6-8ae8-0842dcbf1f36"
   },
   "outputs": [],
   "source": [
    "data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0b19271-ec99-4aa5-8d87-151c76580e85",
    "_uuid": "bd417589-8cd4-4f4d-a842-adbef00b01a4"
   },
   "outputs": [],
   "source": [
    "data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "564fb2c8-8c50-4845-8302-bf894ca6f2f4",
    "_uuid": "aebf4a5a-c219-4fc3-a298-c8ea7502669f"
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9f26868-d446-4446-a469-e94ff8467f46",
    "_uuid": "f8a8e671-982b-48cf-84c4-68bbde4ef65e"
   },
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6e4c6880-6dc9-4932-a2b1-18bc78d71633",
    "_uuid": "27390bdd-c240-4b42-bddf-2bf6f0505ef1"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d4482c90-38ba-424e-83ff-4905c4bcb153",
    "_uuid": "250d539f-1e99-4982-a2b5-19cf978ee541"
   },
   "outputs": [],
   "source": [
    "data_dupli = data.drop_duplicates()\n",
    "data=data_dupli\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62ab1588-a2c1-46a2-9ddd-9a7418a4f28b",
    "_uuid": "da4e01ef-d29f-428b-ab2d-d12f7117a162"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "data['Class']=data['Class'].astype(str)\n",
    "le = LabelEncoder()\n",
    "data['Class'] = le.fit_transform(data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "59dc5b12-6f3d-44b1-9439-082d4d0f7eaa",
    "_uuid": "774cdbdd-3701-4eaa-973e-a0e6fc95e9a7"
   },
   "outputs": [],
   "source": [
    "data['Class'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9e56d1e-1adf-4efb-be79-585a8357f4ca",
    "_uuid": "571a4234-46f5-455b-9735-08ff1dfc087b"
   },
   "outputs": [],
   "source": [
    "import kernelprocess\n",
    "\n",
    "df = kernelprocess.processing(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "file = shuffle(df, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.DataFrame(file['Class'])\n",
    "X = pd.DataFrame(file['Comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "        fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "        # summarize history for accuracy\n",
    "        axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "        axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "        axs[0].set_title('Model Accuracy')\n",
    "        axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "        axs[0].legend(['train', 'test'], loc='best')\n",
    "        # summarize history for loss\n",
    "        axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "        axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "        axs[1].set_title('Model Loss')\n",
    "        axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "        axs[1].legend(['train', 'test'], loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None, split=' ',lower=False)\n",
    "tokenizer.fit_on_texts(X['Comments'].values)\n",
    "x = tokenizer.texts_to_sequences(X['Comments'].values)\n",
    "wordindex = tokenizer.word_index\n",
    "vocab_size = len(wordindex) + 1\n",
    "token = pad_sequences(x, padding  = 'post', maxlen = 2303) #max comment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "\n",
    "# load whole embedding into memory\n",
    "print('Indexing word vectors.')\n",
    "    \n",
    "embeddings_index = {}\n",
    "f = file_io.FileIO('../input/finaldata/glove.6B.100d.txt', mode='r')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "    \n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "#emb_dim could be a parameter\n",
    "    \n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean() \n",
    "emb_std = all_embs.std() \n",
    "emb_mean,emb_std\n",
    "emb_dim=100\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim ))\n",
    "for word, i in wordindex.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = KFold(n_splits=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def CNNmodel(x_train, x_test, y_train, y_test, vocab_size, embedding_matrix):\n",
    "y=Y['Class'].values  \n",
    "cvscores = []\n",
    "for train, test in cv.split(token):  \n",
    "    #print(\"TRAIN:\", train, \"Test:\", test)\n",
    "    Xtrain, Xtest = token[train], token[test]\n",
    "    Ytrain, Ytest = y[train], y[test]\n",
    "    #ramdomOverSample\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    ros = RandomOverSampler()\n",
    "    x_ros, y_ros = ros.fit_sample(Xtrain, Ytrain)   \n",
    "    \n",
    "    seed = 206#10 was good\n",
    "    np.random.seed(seed)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = x_ros.shape[1]))\n",
    "    #model.add(GlobalMaxPooling1D())\n",
    "    model.add(LSTM(102,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    model.add(LSTM(92,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    #model.add(LSTM(100,return_sequences=True, recurrent_dropout=0.6, activation='tanh'))#return_sequences=True\n",
    "    model.add(Conv1D(74, kernel_size=4, activation='tanh'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20,activation = 'tanh', kernel_regularizer = regularizers.l2(0.02)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', mean_pred])\n",
    "    print(model.summary())\n",
    "    batch_size = 110\n",
    "    epochs = 10\n",
    "        \n",
    "    model_history = model.fit(x_ros, y_ros, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)#, callbacks=[EarlyStopping(monitor='val_loss',mode='min',patience=3,min_delta=0.01)])\n",
    "    \n",
    "    LCNN = model.predict_classes(Xtest)\n",
    "    \n",
    "scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "cvscores.append(scores[1] * 100)\n",
    "\n",
    "plot_model_history(model_history)\n",
    "print(\"-----RandomOverSampling-----\")\n",
    "print(\"LCNN Accuracy Score -> \",(np.mean(cvscores)))\n",
    "print(classification_report(Ytest, LCNN))\n",
    "print(confusion_matrix(Ytest,LCNN))\n",
    "import json\n",
    "from keras.models import model_from_json, load_model\n",
    "\n",
    "model.save_weights('LCNNmodelweights.h5')\n",
    "with open('LCNNmodel_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "cvscores = []\n",
    "for train, test in cv.split(token):  \n",
    "    #print(\"TRAIN:\", train, \"Test:\", test)\n",
    "    Xtrain, Xtest = token[train], token[test]\n",
    "    Ytrain, Ytest = y[train], y[test]\n",
    "    #smote\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(random_state=2)\n",
    "    X_train_res, y_train_res = sm.fit_sample(Xtrain, Ytrain)    \n",
    "    \n",
    "    seed = 206#10 was good\n",
    "    np.random.seed(seed)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = X_train_res.shape[1]))\n",
    "    #model.add(GlobalMaxPooling1D())\n",
    "    model.add(LSTM(102,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    model.add(LSTM(92,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    #model.add(LSTM(100,return_sequences=True, recurrent_dropout=0.6, activation='tanh'))#return_sequences=True\n",
    "    model.add(Conv1D(74, kernel_size=4, activation='tanh'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20,activation = 'tanh', kernel_regularizer = regularizers.l2(0.02)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss= 'binary_crossentropy',metrics=(['accuracy',mean_pred]))#sentropy',loss= imbalancedloss\n",
    "    model.summary()\n",
    "    \n",
    "    batch_size = 110\n",
    "    epochs =4 \n",
    "        \n",
    "    model_history = model.fit(X_train_res,y_train_res, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    LCNN = model.predict_classes(Xtest)\n",
    "        \n",
    "scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "cvscores.append(scores[1] * 100)\n",
    "plot_model_history(model_history)\n",
    "print(\"-----SMOTE-----\")\n",
    "print(\"LCNN Accuracy Score -> \", (np.mean(cvscores)))\n",
    "\n",
    "print(classification_report(Ytest, LCNN))\n",
    "print(confusion_matrix(Ytest,LCNN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "cvscores = []\n",
    "for train, test in cv.split(token):  \n",
    "    #print(\"TRAIN:\", train, \"Test:\", test)\n",
    "    Xtrain, Xtest = token[train], token[test]\n",
    "    Ytrain, Ytest = y[train], y[test]\n",
    "    #ramdomUnderSample\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    rus = RandomUnderSampler()\n",
    "    x_rus, y_rus = rus.fit_sample(Xtrain, Ytrain)  \n",
    "    \n",
    "    seed = 206#10 was good\n",
    "    np.random.seed(seed)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = x_rus.shape[1]))\n",
    "    #model.add(GlobalMaxPooling1D())\n",
    "    model.add(LSTM(102,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    model.add(LSTM(92,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    #model.add(LSTM(100,return_sequences=True, recurrent_dropout=0.6, activation='tanh'))#return_sequences=True\n",
    "    model.add(Conv1D(74, kernel_size=4, activation='tanh'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20,activation = 'tanh', kernel_regularizer = regularizers.l2(0.02)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss= 'binary_crossentropy',metrics=(['accuracy',mean_pred]))#sentropy',loss= imbalancedloss\n",
    "    model.summary()\n",
    "    \n",
    "    batch_size = 110\n",
    "    epochs = 10\n",
    "        \n",
    "    model_history = model.fit(x_rus, y_rus, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    LCNN = model.predict_classes(Xtest)\n",
    "        \n",
    "scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "cvscores.append(scores[1] * 100)\n",
    "plot_model_history(model_history)\n",
    "print(\"-----RandomUnderSampling-----\")\n",
    "print(\"LCNN Accuracy Score -> \", (np.mean(cvscores)))\n",
    "\n",
    "print(classification_report(Ytest, LCNN))\n",
    "print(confusion_matrix(Ytest,LCNN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "cvscores = []\n",
    "for train, test in cv.split(token,y):  \n",
    "    #print(\"TRAIN:\", train, \"Test:\", test)\n",
    "    Xtrain, Xtest = token[train], token[test]\n",
    "    Ytrain, Ytest = y[train], y[test]\n",
    "     \n",
    "    \n",
    "    seed = 206#10 was good\n",
    "    np.random.seed(seed)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = Xtrain.shape[1]))\n",
    "    #model.add(GlobalMaxPooling1D())\n",
    "    model.add(LSTM(102,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    model.add(LSTM(92,return_sequences=True, activation='tanh'))#return_sequences=True\n",
    "    #model.add(LSTM(100,return_sequences=True, recurrent_dropout=0.6, activation='tanh'))#return_sequences=True\n",
    "    model.add(Conv1D(74, kernel_size=4, activation='tanh'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20,activation = 'tanh', kernel_regularizer = regularizers.l2(0.02)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss= 'binary_crossentropy',metrics=(['accuracy',mean_pred]))#sentropy',loss= imbalancedloss\n",
    "    model.summary()\n",
    "    \n",
    "    batch_size = 110\n",
    "    epochs = 10\n",
    "        \n",
    "    model_history = model.fit(Xtrain,Ytrain, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    LCNN = model.predict_classes(Xtest)\n",
    "        \n",
    "scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "cvscores.append(scores[1] * 100)\n",
    "plot_model_history(model_history)\n",
    "print(\"-----StratifiedKFold-----\")\n",
    "print(\"LCNN Accuracy Score -> \", (np.mean(cvscores)))\n",
    "\n",
    "print(classification_report(Ytest, LCNN))\n",
    "print(confusion_matrix(Ytest,LCNN))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
