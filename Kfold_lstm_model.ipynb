{"cells":[{"metadata":{"_uuid":"5a8550ea-097d-403c-91c1-3c3b5244d194","_cell_guid":"4640f2bd-c229-42a0-b180-e266d6958029","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, Dense, Embedding, Dropout,MaxPooling1D, LSTM\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nfrom keras.callbacks import EarlyStopping\n\nfrom keras import regularizers\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report, f1_score\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport tensorflow \nfrom tensorflow.python.lib.io import file_io\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c16df588-b323-4902-bf21-d53eb3520920","_cell_guid":"dc2d0fab-c59a-4c93-ad61-d30b1b6a9929","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/finaldata/YouTube_Dadvar_2014.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ae3ad68-a588-4a36-859d-049b6a0cda55","_cell_guid":"dad535a7-3333-40e0-b027-4c77e5798f74","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52364acd-c989-469c-a39d-be61ba89a97a","_cell_guid":"079ed963-a712-436a-9848-1da1832d71b0","trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"108950d8-dabe-49a6-8ae8-0842dcbf1f36","_cell_guid":"8bc19bb0-5214-4089-ae08-aabd0438197d","trusted":true},"cell_type":"code","source":"data.ndim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd417589-8cd4-4f4d-a842-adbef00b01a4","_cell_guid":"c0b19271-ec99-4aa5-8d87-151c76580e85","trusted":true},"cell_type":"code","source":"data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aebf4a5a-c219-4fc3-a298-c8ea7502669f","_cell_guid":"564fb2c8-8c50-4845-8302-bf894ca6f2f4","trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8a8e671-982b-48cf-84c4-68bbde4ef65e","_cell_guid":"b9f26868-d446-4446-a469-e94ff8467f46","trusted":true},"cell_type":"code","source":"data.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27390bdd-c240-4b42-bddf-2bf6f0505ef1","_cell_guid":"6e4c6880-6dc9-4932-a2b1-18bc78d71633","trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"250d539f-1e99-4982-a2b5-19cf978ee541","_cell_guid":"d4482c90-38ba-424e-83ff-4905c4bcb153","trusted":true},"cell_type":"code","source":"data_dupli = data.drop_duplicates()\ndata=data_dupli\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da4e01ef-d29f-428b-ab2d-d12f7117a162","_cell_guid":"62ab1588-a2c1-46a2-9ddd-9a7418a4f28b","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndata['Class']=data['Class'].astype(str)\nle = LabelEncoder()\ndata['Class'] = le.fit_transform(data['Class'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"774cdbdd-3701-4eaa-973e-a0e6fc95e9a7","_cell_guid":"59dc5b12-6f3d-44b1-9439-082d4d0f7eaa","trusted":true},"cell_type":"code","source":"data['Class'].value_counts().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"571a4234-46f5-455b-9735-08ff1dfc087b","_cell_guid":"f9e56d1e-1adf-4efb-be79-585a8357f4ca","trusted":true},"cell_type":"code","source":"import kernelprocess\n\ndf = kernelprocess.processing(data)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\nfile = shuffle(df, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.DataFrame(file['Class'])\nX = pd.DataFrame(file['Comments'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model_history(model_history):\n        fig, axs = plt.subplots(1,2,figsize=(15,5))\n        # summarize history for accuracy\n        axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n        axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n        axs[0].set_title('Model Accuracy')\n        axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n        axs[0].legend(['train', 'test'], loc='best')\n        # summarize history for loss\n        axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n        axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n        axs[1].set_title('Model Loss')\n        axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n        axs[1].legend(['train', 'test'], loc='best')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=None, split=' ',lower=False)\ntokenizer.fit_on_texts(X['Comments'].values)\nx = tokenizer.texts_to_sequences(X['Comments'].values)\nwordindex = tokenizer.word_index\nvocab_size = len(wordindex) + 1\ntoken = pad_sequences(x, padding  = 'post', maxlen = 2303) #max comment length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\n\n# load whole embedding into memory\nprint('Indexing word vectors.')\n    \nembeddings_index = {}\nf = file_io.FileIO('../input/finaldata/glove.6B.100d.txt', mode='r')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n    \nprint('Found %s word vectors.' % len(embeddings_index))\n#emb_dim could be a parameter\n    \nall_embs = np.stack(embeddings_index.values())\nemb_mean = all_embs.mean() \nemb_std = all_embs.std() \nemb_mean,emb_std\nemb_dim=100\nembedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim ))\nfor word, i in wordindex.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\ncv = KFold(n_splits=4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def CNNmodel(x_train, x_test, y_train, y_test, vocab_size, embedding_matrix):\ny=Y['Class'].values  \ncvscores = []\nfor train, test in cv.split(token):  \n    #print(\"TRAIN:\", train, \"Test:\", test)\n    Xtrain, Xtest = token[train], token[test]\n    Ytrain, Ytest = y[train], y[test]\n    #ramdomOverSample\n    from imblearn.over_sampling import RandomOverSampler\n    ros = RandomOverSampler()\n    x_ros, y_ros = ros.fit_sample(Xtrain, Ytrain)   \n    \n    seed = 121\n    np.random.seed(seed)\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = x_ros.shape[1]))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    #model.add(Dropout(0.4))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    model.add(Dropout(0.3))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dropout(0.5))\n    model.add(Dense(20, activation='tanh', kernel_regularizer = regularizers.l2(0.02)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy',mean_pred])\n    print(model.summary())\n    batch_size = 110\n    epochs = 10\n    \n    model_history = model.fit(x_ros, y_ros, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)\n    y_pred = model.predict_classes(Xtest)\n    \nscores = model.evaluate(Xtest, Ytest, verbose=0)\ncvscores.append(scores[1] * 100)\n\nplot_model_history(model_history)\nprint(\"-----RandomOverSampling-----\")\nprint(\"LSTM Accuracy Score -> \",(np.mean(cvscores)))\nprint(classification_report(Ytest, y_pred))\nprint(confusion_matrix(Ytest,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \ncvscores = []\nfor train, test in cv.split(token):  \n    #print(\"TRAIN:\", train, \"Test:\", test)\n    Xtrain, Xtest = token[train], token[test]\n    Ytrain, Ytest = y[train], y[test]\n    #smote\n    from imblearn.over_sampling import SMOTE\n    sm = SMOTE(random_state=2)\n    X_train_res, y_train_res = sm.fit_sample(Xtrain, Ytrain)    \n    \n    seed = 121\n    np.random.seed(seed)\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = X_train_res.shape[1]))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    #model.add(Dropout(0.4))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    model.add(Dropout(0.3))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dropout(0.5))\n    model.add(Dense(20, activation='tanh', kernel_regularizer = regularizers.l2(0.02)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy',mean_pred])\n    print(model.summary())\n    batch_size = 110\n    epochs = 10\n\n    model_history = model.fit(X_train_res, y_train_res, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)\n    y_pred = model.predict_classes(Xtest)\n    \nscores = model.evaluate(Xtest, Ytest, verbose=0)\ncvscores.append(scores[1] * 100)\n\nplot_model_history(model_history)\nprint(\"-----SMOTE-----\")\nprint(\"LSTM Accuracy Score -> \",(np.mean(cvscores)))\nprint(classification_report(Ytest, y_pred))\nprint(confusion_matrix(Ytest,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \ncvscores = []\nfor train, test in cv.split(token):  \n    #print(\"TRAIN:\", train, \"Test:\", test)\n    Xtrain, Xtest = token[train], token[test]\n    Ytrain, Ytest = y[train], y[test]\n    #ramdomUnderSample\n    from imblearn.under_sampling import RandomUnderSampler\n\n    rus = RandomUnderSampler()\n    x_rus, y_rus = rus.fit_sample(Xtrain, Ytrain)  \n   \n    seed = 121\n    np.random.seed(seed)\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = x_rus.shape[1]))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    #model.add(Dropout(0.4))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    model.add(Dropout(0.3))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dropout(0.5))\n    model.add(Dense(20, activation='tanh', kernel_regularizer = regularizers.l2(0.02)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy',mean_pred])\n    print(model.summary())\n    batch_size = 110\n    epochs = 10\n    \n    model_history = model.fit(x_rus, y_rus, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)\n    y_pred = model.predict_classes(Xtest)\n    \nscores = model.evaluate(Xtest, Ytest, verbose=0)\ncvscores.append(scores[1] * 100)\n\nplot_model_history(model_history)\nprint(\"-----RandomUnderSampling-----\")\nprint(\"LSTM Accuracy Score -> \",(np.mean(cvscores)))\nprint(classification_report(Ytest, y_pred))\nprint(confusion_matrix(Ytest,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\ncv = StratifiedKFold(n_splits=4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \ncvscores = []\nfor train, test in cv.split(token,y):  \n    #print(\"TRAIN:\", train, \"Test:\", test)\n    Xtrain, Xtest = token[train], token[test]\n    Ytrain, Ytest = y[train], y[test]\n   \n    seed = 121\n    np.random.seed(seed)\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, input_length = Xtrain.shape[1]))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    #model.add(Dropout(0.4))\n    model.add(LSTM(180, activation='tanh', return_sequences=True))\n    model.add(Dropout(0.3))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dropout(0.5))\n    model.add(Dense(20, activation='tanh', kernel_regularizer = regularizers.l2(0.02)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy',mean_pred])\n    print(model.summary())\n    batch_size = 110\n    epochs = 10\n    \n    model_history = model.fit(Xtrain, Ytrain, validation_data=(Xtest, Ytest),epochs=epochs, batch_size=batch_size, verbose=1)\n    y_pred = model.predict_classes(Xtest)\n\nscores = model.evaluate(Xtest, Ytest, verbose=0)\ncvscores.append(scores[1] * 100)\nplot_model_history(model_history)\nprint(\"-----StratifiedKFold-----\")\nprint(\"LSTM Accuracy Score -> \",(np.mean(cvscores)))\nprint(classification_report(Ytest, y_pred))\nprint(confusion_matrix(Ytest,y_pred))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}